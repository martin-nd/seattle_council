---
title: "Investigating the Tendencies of the Seattle City Council"
author: "Martin Sloley"
output:
    bookdown::html_document2:
        toc: true
        theme: united
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(plotly)
library(caret)
library(reshape2)
library(magrittr)
library(DT)
data = read_csv("scaled_clean_tendency_data.csv")
normdat = read_csv("norm_scaled_clean_tendency_data.csv")
```

```{python setup2, includ=F}
from statsmodels.multivariate.manova import MANOVA
import polars as pl
import pandas as pd
from polars import col as c
import re
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
data = pl.read_csv("scaled_clean_tendency_data.csv")
newcols = ["_".join(re.split("[- ]", colname)) for colname in data.columns]
data.columns = newcols
normdat = pl.read_csv("norm_scaled_clean_tendency_data.csv")
normdat.columns = newcols
```

# Overview

## Goals

One problem with public policy (for me, as an extremely lazy person) is the amount of reading required to get an overview of what matters to local political figures. In order to get a better idea of what the Seattle City Council members consider important, I figured that I could just see what they seem to talk about often, with the goal being putting together a good idea of what matters to each council member and having a dataset that I can draw some conclusions from regarding how I may want to vote in the future.

## Data collection

In order to get a good idea of what council members find important, I figured it best to go straight to the source. Seattle local government offers a [news RSS feed](https://news.seattle.gov) that is freely accessible. I used python's `feedparser` library to grab all RSS feed entries from the last 10 years and stored them in a local Mongo database. Each RSS entry comes with a date, a link to the article or post, and some other metadata. I then sifted through the 20248 entries as of May 15, 2024 and identified every feed entry with a link to `council.seattle.gov`.

Using python's `BeautifulSoup` library, I was able to scrape the raw text of each of these council announcements and put those into a separate collection in the same Mongo database. Then, using regex, I extracted the council member whose office published the announcement for each announcement. After finding all this, I used `nltk` to remove stopwords and create frequency distributions for word count to get a sense of the subjects of announcements. Using this frequency distributions I came up with the following categories for each announcement:

- Budget
- Traffic
- Public transit
- Utilities-light
- Utilities-heat
- Utilities-water
- Utilities-electric
- Civil unrest
- Civil rights
- Parks
- Labor
- Housing
- Homelessness
- Political-legislation
- Political-city council
- Political-mayor
- Political-general
- Crime
- Police-general
- Police-controversy
- Police-political
- Public safety
- Public health
- Public services
- Covid-general
- Covid-vaccines
- Covid-masks
- Infrastructure
- Business
- Real estate
- Welfare-unemployment
- Welfare-food
- Economics
- Environmental
- Sports

After determining these categories, I proceeded to use a zero-shot, deep-learning text classification model to measure the strength of the relationship between these categories and each announcement. I used an open source model and ran it locally. The specific model is available [here](https://huggingface.co/BAAI/bge-reranker-v2-m3). I used normalized outputs and measured the strength of the relationship for all categories for each announcement. I then transformed these measurements to better adhere to Gaussian distributions using the transformation: $\mathcal{G} = \frac{1}{2}\log{\left(\frac{p}{1-p}\right)}$. After transforming I centered and scaled it to get data that looks like this:

```{r display data, echo=F, message=F, warning=F}
DT::datatable(
  data %>% select(-date),
  extensions = c('FixedColumns', "Buttons"),
  options = list(
    scrollX = TRUE,
    scrollY = TRUE,
    fixedColumns = list(leftColumns = 2)
  )
)
```

All code used is available in the [github repo](https://github.com/martin-nd/seattle_council), although, be warned, the collection and processing is extremely messy. If this wasn't just a one-time scrape I would have been cleaner.

# A note on the use of deep learning

As many who know me will attest, I am the first to dismiss the capabilities of deep learning as more hype than substance. I still hold this position (at least until the public's expectations of deep learning capabilities align with their true capabilities), and my use of deep learning here will not go unconsidered. Something to keep in mind throughout this whole investigation from this point forward, is that all these data points were generated by the model's embedding of the tokens that make up the category, and the tokens that make up the article. The model has no actual semantic knowledge of the world, and we will address that; both in how that rears its head in the data, and how that may affect our conclusions.

Before we go any further, let me show you an example. You may have noticed that some of the subjects I looked for were covid-related, but I have data from 2014-2024. Let's see how the model characterized announcements from pre-covid.

```{r covid relevancy graph, fig.width = 10, fig.height = 8}
ggplot(data, aes(x = date, y = `covid-general`, color = councilmember)) +
        geom_point(size = 0.9, alpha = 0.7) +
        labs(title = "Covid Related Announcement Prevalency Over Time",
             y = "Prevalence Score", x = "Date", color = "Council Member") +
        theme(legend.text = element_text(size = 8),
              legend.title = element_text(size = 10))
```

As you can see, there's a legitimate argument that anything under 1 is just random noise. We can combat this by setting anything under 1 to 0, and subtracting 1 from the remaining values. That way there's a minimum value that we can consider as meaning that the subject of the announcement and the subject tested are orthogonal. Unless otherwise stated, assume the use of this sparse dataset moving forward:

```{r}
DT::datatable(
  normdat %>% select(-date),
  extensions = c('FixedColumns', "Buttons"),
  options = list(
    scrollX = TRUE,
    scrollY = TRUE,
    fixedColumns = list(leftColumns = 2)
  )
)
```

# Initial observations

The first step we should take is making sure that the council members are actually talking about different things. We can do this using a MANOVA test.

```{python manova test}
formula_exog = " + ".join(normdat.drop(["councilmember", "date"]).columns)
mv_test = MANOVA.from_formula(f"councilmember ~ {formula_exog}", normdat.drop("date").to_pandas())
test = mv_test.mv_test().results
results_table = {
    "subject": list(test.keys())[1:],
    "pval": []
}
for var in results_table["subject"]:
    pval = test[var]["stat"].iloc[2, 4]
    results_table["pval"].append(pval)

pd.DataFrame(results_table).sort_values(by = "pval", ascending = False).reset_index(drop = True)
```

It would appear that the members of the city council are not always talking about the same things. I would say that's good, it means there's a diversity of thought within the council. Let's get some aggregates and see what each council member is talking about. We can do this with a parallel coordinates plot.

```{r, warning=F, message=F, fig.width=10, fig.height=8}
norm_melted = normdat %>%
        select(-date) %>%
        group_by(councilmember) %>%
        summarize_if(is.numeric, mean) %>%
        melt()

ggplot(norm_melted, aes(x = variable, y = value, color = councilmember, group = councilmember)) +
        geom_line() +
        labs(title = "Parallel Coordinates Plot of Subject Prevalence per Council Member",
             y = "Prevalence Score", x = "Subject", color = "Council Member") +
        theme(axis.text.x = element_text(size = 9, angle = 45))
```


We can also compare subsets, for example, the three most prevalent council members in the data set.

```{r top3, message=F, warning=F, fig.width = 10, fig.height = 8}
top3 = normdat %>%
        group_by(councilmember) %>%
        summarize(count = n()) %>%
        arrange(desc(count)) %>%
        head(3) %>%
        .$councilmember

top3_melt = norm_melted %>%
        filter(councilmember %in% top3)

ggplot(top3_melt, aes(x = variable, y = value, color = councilmember, group = councilmember)) +
  geom_line() +
  labs(title = "Parallel Coordinates Plot of Subject Prevalence for 3 Most Prolific Council Members",
       y = "Prevalence Score", x = "Subject", color = "Council Member") +
  theme(axis.text.x = element_text(size = 9, angle = 45))
```

We can see here that council members Herbold and Sawant tend not to talk about the same subjects.

# Unsupervised Learning

Let's see if there's any clusters of posts, and whether any particular council members have outsized prevalence in those clusters. We will do this with a sparse PCA transformation and then observing in 3 reduced dimensions.

```{r spca, warning=F, message=F}
library(elasticnet)
numbers_only = normdat[, 3:ncol(normdat)]
sparse_loadings = spca(numbers_only,
                       K = 3,
                       sparse = "penalty",
                       para = rep(0.5, 3),
                       type = "predictor")
sparse_matrix = as.data.frame(as.matrix(numbers_only) %*% sparse_loadings$loadings)
sparse_matrix$date = normdat$date
sparse_matrix$councilmember = normdat$councilmember

plot_ly(data = sparse_matrix,
        type = "scatter3d",
        x = ~PC1, y = ~PC2, z = ~PC3,
        size = 0.1, color = ~councilmember) %>%
        layout(title = "Announcement Embeddings in 3 Dimensions")
```

That is certainly an interesting and unusual underlying structure, but I think there is a simple explanation for this. This is a natural consequence of using deep learning in the way that we did. By using a model to estimate the strength of the association of each announcement with a number of categories, we are creating a unit hypercube space in which each announcement is embedded. By reducing the dimensionality of the hypercube, we create an rotation of a standard 3-dimensional cube, in which each post is embedded. Since announcements from the city council are presumably usually about one main subject, most announcements are embedded along the vertices of the hypercube, and subsequently along the vertices of the cube as well. This is likely made more clear by the sparse transformation, as the regularization zeroes out the weaker relationships of each announcement with other categories.

Unfortunately this structure doesn't indicate that any kind of clustering might be useful, but we may still be able to gain some insight from the embeddings.

Let's do the same as we did previously and examine just Herbold, Sawant, and Mosqueda.

*Tip: click on a name in the legend to hide / show announcements from that council member*

```{r hsm embeddings, warning=F, message=F}
hsm_sparse_matrix = sparse_matrix %>%
        filter(councilmember %in% c("Herbold", "Mosqueda", "Sawant"))

plot_ly(data = hsm_sparse_matrix,
        type = "scatter3d",
        x = ~PC1, y = ~PC2, z = ~PC3,
        size = 0.1, color = ~councilmember) %>%
  layout(title = list(text = "Announcement Embeddings in 3 Dimensions\nHerbold, Sawant, Mosqueda only", x = 0.1))
```

We can see that each of the three most prevalent council members have their own strong and weak areas when it comes to announcements. Sawant and Herbold each have strong presences in two axes, while Mosqueda's announcements are mostly concentrated in a single axis. We can use the sparse loadings to get an idea of what these subjects are.

For Mosqueda, we look for subjects with PC3 and PC2 loadings of close to 0.

```{r}
qs3 = quantile(abs(sparse_loadings$loadings[, 3]))
qs2 = quantile(abs(sparse_loadings$loadings[, 2]))
qs1 = quantile(abs(sparse_loadings$loadings[, 1]))
sparse_loadings$loadings %>%
        as.data.frame() %>%
        mutate(variable = colnames(numbers_only)) %>%
        filter(abs(PC3) < qs3[2] & abs(PC2) < qs2[2]) %>%
        .$variable
```

For Sawant, we can look at subjects with just a PC3 of close to 0
```{r}
sparse_loadings$loadings %>%
  as.data.frame() %>%
  mutate(variable = colnames(numbers_only)) %>%
  filter(abs(PC3) < qs3[2]) %>%
  .$variable
```

And for Herbold we can look at subjects with a PC1 of close to 0.

```{r}
sparse_loadings$loadings %>%
  as.data.frame() %>%
  mutate(variable = colnames(numbers_only)) %>%
  filter(abs(PC1) < qs1[2]) %>%
  .$variable
```

# Align Me

The last thing I'd like to do with this data is make a tool. This tool will take into account this data of what these council members talk about, allow users to input how important certain issues are to them on a sliding scale, and then match those users with council members that are making announcements concerning the subjects that those users care about. Keep in mind that the deep learning model is only measuring the strength of the relationship between the announcement and the proposed category, it makes no judgement on the nuance of those council members' positions on those issues. The tool is currently in development, but will be available soon, linked here and on my website (which is probably how you got here).