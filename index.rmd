---
title: "Investigating the Tendencies of the Seattle City Council"
author: "Martin Sloley"
output: bookdown::html_document2
---

```{r setup, include=F}
library(tidyverse)
library(plotly)
library(caret)
```

# Overview

## Goals

One problem with public policy (for me, as an extremely lazy person) is the amount of reading required to get an overview of what matters to local political figures. In order to get a better idea of what the Seattle City Council members consider important, I figured that I could just see what they seem to talk about often, with the goal being putting together a good idea of what matters to each council member and having a dataset that I can draw some conclusions from regarding how I may want to vote in the future.

## Data collection

In order to get a good idea of what council members find important, I figured it best to go straight to the source. Seattle local government offers a [news RSS feed](https://news.seattle.gov) that is freely accessible. I used python's `feedparser` library to grab all RSS feed entries from the last 10 years and stored them in a local Mongo database. Each RSS entry comes with a date, a link to the article or post, and some other metadata. I then sifted through the 20248 entries as of May 15, 2024 and identified every feed entry with a link to `council.seattle.gov`.

Using python's `BeautifulSoup` library, I was able to scrape the raw text of each of these council announcements and put those into a separate collection in the same Mongo database. Then, using regex, I extracted the council member whose office published the announcement for each announcement. After finding all this, I used `nltk` to remove stopwords and create frequency distributions for word count to get a sense of the subjects of announcements. Using this frequency distributions I came up with the following categories for each announcement:

- Budget
- Traffic
- Public transit
- Utilities-light
- Utilities-heat
- Utilities-water
- Utilities-electric
- Civil unrest
- Civil rights
- Parks
- Labor
- Housing
- Homelessness
- Political-legislation
- Political-city council
- Political-mayor
- Political-general
- Crime
- Police-general
- Police-controversy
- Police-political
- Public safety
- Public health
- Public services
- Covid-general
- Covid-vaccines
- Covid-masks
- Infrastructure
- Business
- Real estate
- Welfare-unemployment
- Welfare-food
- Economics
- Environmental
- Sports

After determining these categories, I proceeded to use a zero-shot, deep-learning text classification model to measure the strength of the relationship between these categories and each announcement. I used an open source model and ran it locally. The specific model is available [here](https://huggingface.co/BAAI/bge-reranker-v2-m3). I used normalized outputs and measured the strength of the relationship for all categories for each announcement. I then transformed these measurements to better adhere to Gaussian distributions using the transformation: $\mathcal{G} = \frac{1}{2}\log{\left(\frac{p}{1-p}\right)}$. After transforming I centered and scaled it to get data that looks like this:

```{r display data, echo=F, message=F, warning=F}
data = read_csv("scaled_clean_tendency_data.csv")
head(data, 10)
```

All code used is available in the [github repo](https://github.com/martin-nd/seattle_council), although, be warned, the collection and processing is extremely messy. If this wasn't just a one-time scrape I would have been cleaner.

# A note on the use of deep learning

As many who know me will attest, I am the first to dismiss the capabilities of deep learning as more hype than substance. I still hold this position (at least until the public's expectations of deep learning capabilities align with their true capabilities), and my use of deep learning here will not go unconsidered. Something to keep in mind throughout this whole investigation from this point forward, is that all these data points were generated by the model's embedding of the tokens that make up the category, and the tokens that make up the article. The model has no actual semantic knowledge of the world, and we will address that; both in how that rears its head in the data, and how that may affect our conclusions.

